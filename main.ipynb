{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":15282,"databundleVersionId":565187,"sourceType":"competition"},{"sourceId":9418254,"sourceType":"datasetVersion","datasetId":5720146}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report\n\n# Veri yolları ve parametreler\nTRAIN_DIR = '/kaggle/input/cleanedvsdirtyplates/train'\nTEST_DIR = '/kaggle/input/cleanedvsdirtyplates/test'\nIMG_SIZE = (224, 224)\nBATCH_SIZE = 32\nEPOCHS = 50\n\ndef create_model():\n    model = Sequential([\n        Conv2D(32, (3, 3), activation='relu', input_shape=(*IMG_SIZE, 3)),\n        MaxPooling2D(2, 2),\n        Conv2D(64, (3, 3), activation='relu'),\n        MaxPooling2D(2, 2),\n        Conv2D(64, (3, 3), activation='relu'),\n        MaxPooling2D(2, 2),\n        Flatten(),\n        Dense(64, activation='relu'),\n        Dropout(0.5),\n        Dense(1, activation='sigmoid')\n    ])\n    \n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\ndef load_and_preprocess_data():\n    train_datagen = ImageDataGenerator(\n        rescale=1./255,\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        validation_split=0.2\n    )\n\n    train_generator = train_datagen.flow_from_directory(\n        TRAIN_DIR,\n        target_size=IMG_SIZE,\n        batch_size=BATCH_SIZE,\n        class_mode='binary',\n        subset='training'\n    )\n\n    validation_generator = train_datagen.flow_from_directory(\n        TRAIN_DIR,\n        target_size=IMG_SIZE,\n        batch_size=BATCH_SIZE,\n        class_mode='binary',\n        subset='validation'\n    )\n\n    return train_generator, validation_generator\n\ndef train_model(model, train_generator, validation_generator):\n    history = model.fit(\n        train_generator,\n        steps_per_epoch=train_generator.samples // BATCH_SIZE,\n        validation_data=validation_generator,\n        validation_steps=validation_generator.samples // BATCH_SIZE,\n        epochs=EPOCHS,\n        callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n    )\n    return history\n\ndef evaluate_model(model, test_generator):\n    predictions = model.predict(test_generator)\n    predicted_classes = (predictions > 0.5).astype(int)\n    true_classes = test_generator.classes\n    class_labels = list(test_generator.class_indices.keys())\n    print(classification_report(true_classes, predicted_classes, target_names=class_labels))\n\ndef predict_test_data(model, test_dir):\n    test_datagen = ImageDataGenerator(rescale=1./255)\n    test_generator = test_datagen.flow_from_directory(\n        test_dir,\n        target_size=IMG_SIZE,\n        batch_size=1,\n        class_mode=None,\n        shuffle=False\n    )\n\n    predictions = model.predict(test_generator)\n    filenames = test_generator.filenames\n    results = []\n\n    for filename, prediction in zip(filenames, predictions):\n        img_id = os.path.splitext(os.path.basename(filename))[0]\n        label = 'cleaned' if prediction[0] > 0.5 else 'dirty'\n        results.append({'id': img_id, 'label': label})\n\n    return results\n\ndef main():\n    model = create_model()\n    train_generator, validation_generator = load_and_preprocess_data()\n    history = train_model(model, train_generator, validation_generator)\n    \n    # Model değerlendirmesi\n    test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n        TEST_DIR,\n        target_size=IMG_SIZE,\n        batch_size=BATCH_SIZE,\n        class_mode='binary',\n        shuffle=False\n    )\n    evaluate_model(model, test_generator)\n    \n    # Test verileri üzerinde tahmin\n    results = predict_test_data(model, TEST_DIR)\n    \n    # Sonuçları kaydet\n    with open('predictions.csv', 'w') as f:\n        f.write('id,label\\n')\n        for result in results:\n            f.write(f\"{result['id']},{result['label']}\\n\")\n\nif __name__ == '__main__':\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-09-17T08:42:15.891202Z","iopub.execute_input":"2024-09-17T08:42:15.891910Z","iopub.status.idle":"2024-09-17T08:42:42.976084Z","shell.execute_reply.started":"2024-09-17T08:42:15.891863Z","shell.execute_reply":"2024-09-17T08:42:42.974285Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Found 32 images belonging to 2 classes.\nFound 8 images belonging to 2 classes.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.4688 - loss: 0.6935 - val_accuracy: 0.5000 - val_loss: 0.7772\nEpoch 2/50\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self.gen.throw(typ, value, traceback)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5000 - val_loss: 0.8162\nEpoch 3/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5312 - loss: 0.8148 - val_accuracy: 0.5000 - val_loss: 0.8854\nEpoch 4/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5000 - val_loss: 0.8711\nEpoch 5/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 1.0160 - val_accuracy: 0.3750 - val_loss: 0.7414\nEpoch 6/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5000 - val_loss: 0.7323\nEpoch 7/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.6562 - loss: 0.6836 - val_accuracy: 0.3750 - val_loss: 0.7481\nEpoch 8/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5000 - val_loss: 0.7268\nEpoch 9/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5625 - loss: 0.6933 - val_accuracy: 0.5000 - val_loss: 0.7656\nEpoch 10/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5000 - val_loss: 0.7594\nEpoch 11/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.4688 - loss: 0.7030 - val_accuracy: 0.5000 - val_loss: 0.7267\nEpoch 12/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5000 - val_loss: 0.7347\nEpoch 13/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5312 - loss: 0.6976 - val_accuracy: 0.3750 - val_loss: 0.7079\nEpoch 14/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5000 - val_loss: 0.6992\nEpoch 15/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5312 - loss: 0.6926 - val_accuracy: 0.5000 - val_loss: 0.6991\nEpoch 16/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.2500 - val_loss: 0.7027\nEpoch 17/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.6250 - loss: 0.6797 - val_accuracy: 0.1250 - val_loss: 0.7092\nEpoch 18/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.1250 - val_loss: 0.7052\nEpoch 19/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.6875 - loss: 0.6815 - val_accuracy: 0.2500 - val_loss: 0.7111\nEpoch 20/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.2500 - val_loss: 0.7027\nFound 0 images belonging to 0 classes.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 127\u001b[0m\n\u001b[1;32m    124\u001b[0m             f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 127\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[2], line 115\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Model değerlendirmesi\u001b[39;00m\n\u001b[1;32m    108\u001b[0m test_generator \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[1;32m    109\u001b[0m     TEST_DIR,\n\u001b[1;32m    110\u001b[0m     target_size\u001b[38;5;241m=\u001b[39mIMG_SIZE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    114\u001b[0m )\n\u001b[0;32m--> 115\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Test verileri üzerinde tahmin\u001b[39;00m\n\u001b[1;32m    118\u001b[0m results \u001b[38;5;241m=\u001b[39m predict_test_data(model, TEST_DIR)\n","Cell \u001b[0;32mIn[2], line 75\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, test_generator)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(model, test_generator):\n\u001b[0;32m---> 75\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     predicted_classes \u001b[38;5;241m=\u001b[39m (predictions \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     77\u001b[0m     true_classes \u001b[38;5;241m=\u001b[39m test_generator\u001b[38;5;241m.\u001b[39mclasses\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/tree/optree_impl.py:76\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`func` must be callable. Received: func=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m structures:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust provide at least one structure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m     78\u001b[0m     assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","\u001b[0;31mValueError\u001b[0m: Must provide at least one structure"],"ename":"ValueError","evalue":"Must provide at least one structure","output_type":"error"}]}]}